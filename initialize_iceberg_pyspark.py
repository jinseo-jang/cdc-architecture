from pyspark.sql import SparkSession
import google.auth
from google.auth.transport.requests import Request
import os

# 1. í™˜ê²½ ì„¤ì •
project_id = "duper-project-1"
location = "us-central1"
catalog_id = "duper-project-1-iceberg-storage"
bucket_name = "duper-project-1-iceberg-storage"
namespace = "iceberg_dataset"
table_name = "products_iceberg"


# 2. ì¸ì¦ í† í° íšë“
def get_access_token():
    credentials, _ = google.auth.default(
        scopes=["https://www.googleapis.com/auth/cloud-platform"]
    )
    credentials.refresh(Request())
    return credentials.token


access_token = get_access_token()

# 3. Spark ì„¸ì…˜ ë¹Œë” (BigLake REST Catalog ì„¤ì •)
# ğŸš€ í•µì‹¬: warehouseë¥¼ 'bq://'ë¡œ ì„¤ì •í•˜ë©´ BigQueryì™€ ìë™ ì—°ë™ë©ë‹ˆë‹¤.
spark = (
    SparkSession.builder.appName("BigLake_Iceberg_Setup")
    .config(f"spark.sql.catalog.{catalog_id}", "org.apache.iceberg.spark.SparkCatalog")
    .config(f"spark.sql.catalog.{catalog_id}.type", "rest")
    .config(
        f"spark.sql.catalog.{catalog_id}.uri",
        "https://biglake.googleapis.com/iceberg/v1/restcatalog",
    )
    .config(
        f"spark.sql.catalog.{catalog_id}.warehouse",
        f"bq://projects/{project_id}/locations/{location}",
    )
    .config(f"spark.sql.catalog.{catalog_id}.token", access_token)
    .config(f"spark.sql.catalog.{catalog_id}.header.x-goog-user-project", project_id)
    .config(
        f"spark.sql.catalog.{catalog_id}.io-impl",
        "org.apache.iceberg.gcp.gcs.GCSFileIO",
    )
    .config(
        "spark.sql.extensions",
        "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions",
    )
    .getOrCreate()
)

# 4. ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë° í…Œì´ë¸” ìƒì„± SQL ì‹¤í–‰
print(f"ğŸš€ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ {namespace} ìƒì„± ì¤‘...")
# ğŸ’¡ catalog_idë¥¼ ë°±í‹±(`)ìœ¼ë¡œ ê°ì‹¸ì•¼ í•©ë‹ˆë‹¤.
spark.sql(f"CREATE NAMESPACE IF NOT EXISTS `{catalog_id}`.{namespace}")

print(f"ğŸš€ í…Œì´ë¸” {table_name} ìƒì„± ì¤‘...")
# ğŸ’¡ ì—¬ê¸°ë„ catalog_idë¥¼ ë°±í‹±(`)ìœ¼ë¡œ ê°ì‹¸ì¤ë‹ˆë‹¤.
spark.sql(
    f"""
    CREATE TABLE IF NOT EXISTS `{catalog_id}`.{namespace}.{table_name} (
        id bigint,
        name string,
        price double,
        updated_at timestamp
    ) 
    USING iceberg
    LOCATION 'gs://{bucket_name}/{namespace}/{table_name}'
"""
)

# 5. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚½ì…
print("ğŸš€ ë°ì´í„° ì‚½ì… ì¤‘...")
# ğŸ’¡ ì—¬ê¸°ë„ ë°±í‹± ì¶”ê°€!
spark.sql(
    f"""
    INSERT INTO `{catalog_id}`.{namespace}.{table_name} 
    VALUES (1, 'pyspark_test_item', 99.9, current_timestamp())
"""
)

print(
    f"âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ! ì´ì œ BigQuery ì½˜ì†”ì—ì„œ '{namespace}.{table_name}'ì„ í™•ì¸í•˜ì„¸ìš”."
)
